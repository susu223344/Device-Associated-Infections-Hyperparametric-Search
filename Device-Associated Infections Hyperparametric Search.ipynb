{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device-Associated Infections Hyperparametric Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train[[\n",
    "'imv','iuc','cvc','gender','myocardial_infarct','malignant_cancer','liver_disease','case_micu/sicu'\n",
    "]]\n",
    "\n",
    "X_test=X_test[[\n",
    "'imv','iuc','cvc','gender','myocardial_infarct','malignant_cancer','liver_disease','case_micu/sicu'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interp\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import PrecisionRecallDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BayesianOptimization for RF LR SVM XGBoost GNB DT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We initially narrowed down the parameters through a manual search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BayesianOptimization code for RF LR SVM XGBoost GNB DT\n",
    "# SVM example  \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "import warnings  \n",
    "warnings.filterwarnings(action='ignore')  \n",
    "\n",
    "\n",
    "def rf_cv(max_iter):\n",
    "    val = cross_val_score(\n",
    "       SVC(probability= True, \n",
    "           kernel='linear',\n",
    "           degree=3,\n",
    "           max_iter=int(max_iter),\n",
    "           random_state=123\n",
    "           ), \n",
    "        X_train, y_train, scoring='neg_mean_squared_error', cv=10\n",
    "    ).mean()\n",
    "    return val\n",
    "\n",
    "# 规定各参数搜索范围\n",
    "rf_bo = BayesianOptimization(rf_cv,\n",
    "                             {'max_iter': (..., ...)\n",
    "                              })\n",
    "rf_bo.maximize()\n",
    "\n",
    "rf_bo.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BayesianOptimization for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "import warnings  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, mean_absolute_error, r2_score  # 模型评估方法\n",
    "import keras.layers as layers  \n",
    "import keras.backend as K  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.layers import LSTM  \n",
    "\n",
    "from tensorflow.keras.utils import plot_model  \n",
    " \n",
    "warnings.filterwarnings(action='ignore')  \n",
    " \n",
    " \n",
    "\n",
    "def bayesopt_objective_lstm(units, epochs):\n",
    "    lstm = Sequential() \n",
    "    lstm.add(LSTM(units=32, return_sequences=True, input_shape=(X_train.shape[1], 1))) \n",
    "    lstm.add(LSTM(units=int(units))) \n",
    "    lstm.add(Dense(10, activation='relu'))  \n",
    "    lstm.add(Dense(1))  \n",
    "    lstm.compile(loss='mean_squared_error',\n",
    "                 optimizer='adam', metrics='mse')  \n",
    "    lstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=int(epochs), batch_size=64)  \n",
    "    score = lstm.evaluate(X_test, y_test, batch_size=128) \n",
    " \n",
    "    return score[1] \n",
    " \n",
    " \n",
    "\n",
    "def param_bayes_opt_lstm(init_points, n_iter):\n",
    "    opt = BayesianOptimization(bayesopt_objective_lstm\n",
    "                               , param_grid_simple\n",
    "                               , random_state=7) \n",
    " \n",
    "  \n",
    "    opt.maximize(init_points=init_points  \n",
    "                 , n_iter=n_iter  \n",
    "                 )\n",
    " \n",
    "    \n",
    "    params_best = opt.max['params'] \n",
    "    score_best = opt.max['target']  \n",
    " \n",
    "    return params_best, score_best \n",
    " \n",
    " \n",
    "\n",
    "def bayes_opt_validation_lstm(params_best):\n",
    "    lstm = Sequential() \n",
    "    lstm.add(LSTM(units=32, return_sequences=True, input_shape=(X_train.shape[1], 1))) \n",
    "    lstm.add(LSTM(units=int(params_best['units'])))\n",
    "    lstm.add(Dense(10, activation='relu')) \n",
    "    lstm.add(Dense(1)) \n",
    "    lstm.compile(loss='mean_squared_error',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['mse']) \n",
    "    lstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=int(params_best['epochs']), batch_size=64) \n",
    " \n",
    "    score = lstm.evaluate(X_test, y_test, batch_size=128) \n",
    " \n",
    "    return score[1] \n",
    " \n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    " \n",
    " \n",
    " \n",
    "    X_train = layers.Lambda(lambda X_trainf: K.expand_dims(X_trainf, axis=-1))(X_trainf) \n",
    " \n",
    "    print('***********************查看训练集的形状**************************')\n",
    "    print(X_train.shape)  \n",
    " \n",
    "    X_test = layers.Lambda(lambda X_testf: K.expand_dims(X_testf, axis=-1))(X_testf)  \n",
    "    print('***********************查看测试集的形状**************************')\n",
    "    print(X_test.shape)  \n",
    " \n",
    "    param_grid_simple = {'units': (..., ...)  \n",
    "        , 'epochs': (..., ...)  \n",
    "                         }\n",
    " \n",
    "    params_best, score_best = param_bayes_opt_lstm(10, 10)  \n",
    " \n",
    "    print('最优参数组合:  ', 'epochs的参数值为：', int(params_best['epochs']), '  units的参数值为：',\n",
    "          int(params_best['units']))\n",
    "    print('最优分数：  ', abs(score_best))  \n",
    "    validation_score = bayes_opt_validation_lstm(params_best)  \n",
    "    print('验证集MSE：  ', abs(validation_score)) \n",
    " \n",
    "  \n",
    "    lstm = Sequential()  \n",
    "    lstm.add(LSTM(units=32, return_sequences=True, input_shape=(X_train.shape[1], 1)))  \n",
    "    lstm.add(LSTM(int(params_best['units']))) \n",
    "    lstm.add(Dense(10, activation='relu')) \n",
    "    lstm.add(Dense(1)) \n",
    "    lstm.compile(loss='mean_squared_error',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['mse']) \n",
    "    history = lstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=int(params_best['epochs']),\n",
    "                       batch_size=64) \n",
    "    print('*************************输出模型摘要信息*******************************')\n",
    "    print(lstm.summary())  \n",
    " \n",
    "    plot_model(lstm, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('最优参数组合:  ', 'epochs的参数值为：', int(params_best['epochs']), '  units的参数值为：',\n",
    "          int(params_best['units'])) \n",
    "print('最优分数：  ', abs(score_best)) \n",
    "print('验证集MSE：  ', abs(validation_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*************************输出模型摘要信息*******************************')\n",
    "print(lstm.summary()) \n",
    " \n",
    "plot_model(lstm, to_file='model.png', show_shapes=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "        loss = history.history['loss']  \n",
    "        val_loss = history.history['val_loss'] \n",
    "        epochs = range(1, len(loss) + 1)  \n",
    "        plt.figure(figsize=(12, 4))  #\n",
    "        plt.plot(epochs, loss, 'r', label='Training loss')  \n",
    "        plt.plot(epochs, val_loss, 'b', label='Test loss')  \n",
    "        plt.title('Training and Test loss')  \n",
    "        plt.xlabel('Epochs') \n",
    "        plt.ylabel('Loss') \n",
    "        plt.legend()\n",
    "        plt.show()  \n",
    " \n",
    " \n",
    "show_history(history) \n",
    " \n",
    "y_pred = lstm.predict(X_test, batch_size=10)  \n",
    " \n",
    "print('----------------模型评估-----------------')\n",
    "\n",
    "print('**************************输出测试集的模型评估指标结果*******************************')\n",
    " \n",
    "print('LSTM回归模型-最优参数-R^2:', round(r2_score(y_test, y_pred), 4))\n",
    "print('LSTM回归模型-最优参数-均方误差:', round(mean_squared_error(y_test, y_pred), 4))\n",
    "print('LSTM回归模型-最优参数-解释方差分:', round(explained_variance_score(y_test, y_pred), 4))\n",
    "print('LSTM回归模型-最优参数-绝对误差:', round(mean_absolute_error(y_test, y_pred), 4))\n",
    " \n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  \n",
    "plt.rcParams['axes.unicode_minus'] = False \n",
    "plt.plot(range(len(y_test)), y_test, color=\"blue\", linewidth=1.5, linestyle=\"-\") \n",
    "plt.plot(range(len(y_pred)), y_pred, color=\"red\", linewidth=1.5, linestyle=\"-.\")  \n",
    "plt.legend(['真实值', '预测值']) \n",
    "plt.title(\"贝叶斯优化器优化LSTM回归模型真实值与预测值比对图\") \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final parameter of the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "RF = RandomForestClassifier(n_estimators = 208, min_samples_split = 2, criterion = \"gini\", min_samples_leaf = 1, \n",
    "                            min_weight_fraction_leaf = 0, max_features = \"sqrt\", min_impurity_decrease = 0)\n",
    "# LR\n",
    "LR =  LogisticRegression(penalty = \"l2\", dual = False, tol = 0.0001, C = 1, verbose = 0)\n",
    "# SVM\n",
    "SVM = SVC(kernel='linear', probability= True, max_iter=787, degree=3, random_state=123, C = 1, tol = 0.001, cache_size = 200)\n",
    "# XGBoost\n",
    "XGBoost = xgb.XGBClassifier(n_estimators=503, gamma=13, learning_rate=0.01, n_jobs=8)\n",
    "# GNB\n",
    "GNB = GaussianNB(var_smoothing=3.9120442541811716e-08)\n",
    "# DT\n",
    "DT = DecisionTreeClassifier(max_depth=136, criterion = \"gini\", splitter = \"best\", min_samples_split = 2, \n",
    "                            min_samples_leaf = 1,min_weight_fraction_leaf = 0, min_impurity_decrease = 0)\n",
    "# LSTM\n",
    "lstm = Sequential() \n",
    "lstm.add(LSTM(units=32, return_sequences=True, input_shape=(X_trainf.shape[1], 1))) \n",
    "lstm.add(LSTM(units=int(53))) \n",
    "lstm.add(Dense(10, activation='relu')) \n",
    "lstm.add(Dense(1))  \n",
    "lstm.compile(loss='mean_squared_error',\n",
    "                 optimizer='sgd', metrics='mse') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival Model Hyperparametric Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train[[\n",
    "'imv',\n",
    "'age',\n",
    "'aniongap',\n",
    "'resp_rate',\n",
    "'tracheostomy',\n",
    "'apsiii',\n",
    "'race_black',\n",
    "'case_micu/sicu'\n",
    "]]\n",
    "\n",
    "X_test=X_test[[\n",
    "'imv',\n",
    "'age',\n",
    "'aniongap',\n",
    "'resp_rate',\n",
    "'tracheostomy',\n",
    "'apsiii',\n",
    "'race_black',\n",
    "'case_micu/sicu'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variable conversion\n",
    "from sksurv.util import Surv\n",
    "# training data\n",
    "y_train_ = Surv.from_dataframe(\n",
    "    event='status', \n",
    "    time='survival_time', \n",
    "    data=y_train)\n",
    "y_train_\n",
    "# testing data\n",
    "y_test_ = Surv.from_dataframe(\n",
    "    event='status', \n",
    "    time='survival_time', \n",
    "    data=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BayesSearchCV for Cox EST ST RSF GBST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BayesSearchCV code for Cox EST ST RSF GBST\n",
    "# EST example\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "search_spaces = {\n",
    "  'n_estimators': Integer(..., ...),\n",
    "  'max_depth': Integer(..., ...) \n",
    "}\n",
    "\n",
    "n_iter_search = 20 \n",
    "bayes_search = BayesSearchCV( \n",
    "    ExtraSurvivalTrees(), \n",
    "    search_spaces, \n",
    "    n_iter=n_iter_search, \n",
    "    cv=10, \n",
    "    verbose=3 \n",
    ") \n",
    "bayes_search.fit(X_train, y_train_)\n",
    "\n",
    "print(\"val. score: %s\" % bayes_search.best_score_)\n",
    "print(\"test score: %s\" % bayes_search.score(X_test, y_test_))\n",
    "print(\"best params: %s\" % str(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BayesianOptimization for DeepSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pysurvival.models.simulations import SimulationModel\n",
    "from pysurvival.models.semi_parametric import NonLinearCoxPHModel\n",
    "from pysurvival.utils.metrics import concordance_index\n",
    "from pysurvival.utils.display import integrated_brier_score\n",
    "\n",
    "# Creating the X, Y and E input\n",
    "Y_train = y_train['survival_time'].values\n",
    "Y_test = y_test['survival_time'].values\n",
    "E_train = (y_train['status'].values).astype(int)\n",
    "E_test = (y_test['status'].values).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Given the extracted code, I will rewrite the Bayesian Optimization functions.\n",
    "\n",
    "# Objective function for Bayesian optimization\n",
    "def bayesopt_objective_deepsurv(unit):\n",
    "    structure = [\n",
    "        {'activation': 'relu', 'num_units': int(unit)}, \n",
    "        {'activation': 'tanh', 'num_units': 32}\n",
    "    ]\n",
    "    # You should train your model here and return the score\n",
    "    deepsurv = NonLinearCoxPHModel(structure=structure)\n",
    "    deepsurv.fit(X_train, Y_train, E_train, lr=1e-3, init_method='xav_uniform', num_epochs=200,dropout=0.8,batch_normalization=True,bn_and_dropout=True)\n",
    "\n",
    "    score = concordance_index(deepsurv, X_test, Y_test, E_test)\n",
    "    return score  \n",
    "    return -unit  # Mock score (negative because BayesianOptimization tries to maximize the objective)\n",
    "\n",
    "# Bayesian optimizer\n",
    "def param_bayes_opt_deepsurv(init_points, n_iter):\n",
    "    opt = BayesianOptimization(bayesopt_objective_deepsurv, {'unit': (..., ...)}, random_state=100, allow_duplicate_points=True)\n",
    "    opt.maximize(init_points=init_points, n_iter=n_iter)\n",
    "    params_best = opt.max['params']\n",
    "    score_best = opt.max['target']\n",
    "    return params_best, score_best\n",
    "\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main_bayesian_optimization():\n",
    "    param_grid_simple = {'unit': (..., ...)}\n",
    "    params_best, score_best = param_bayes_opt_deepsurv(10, 10)\n",
    "    print('Best parameter combination:', 'unit value:', int(params_best['unit']))\n",
    "\n",
    "main_bayesian_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final parameter of the survival model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cox\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis\n",
    "Cox = CoxPHSurvivalAnalysis(alpha = 0, ties = \"breslow\", n_iter = 100, tol = 1e-9, verbose = 0)\n",
    "# EST\n",
    "from sksurv.ensemble import ExtraSurvivalTrees\n",
    "EST = ExtraSurvivalTrees(n_estimators=208, max_depth=100, min_samples_split = 6, min_samples_leaf = 3,\n",
    "                         min_weight_fraction_leaf = 0, max_features = \"sqrt\")\n",
    "# ST\n",
    "from sksurv.tree import SurvivalTree\n",
    "ST = SurvivalTree(max_depth=11, splitter = \"best\", max_depth = None, min_samples_split = 6, \n",
    "                  min_samples_leaf = 3, min_weight_fraction_leaf = 0)\n",
    "# RSF\n",
    "from sksurv.ensemble import RandomSurvivalForest \n",
    "RSF = RandomSurvivalForest(n_estimators=209,  max_depth = None, min_samples_split = 6, min_samples_leaf = 3, \n",
    "                           min_weight_fraction_leaf = 0, max_features = \"sqrt\")\n",
    "# GBST\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "GBST =  GradientBoostingSurvivalAnalysis(n_estimators=204, max_depth=4, loss = \"coxph\", learning_rate = 0.1, criterion = 'friedman_mse',\n",
    "                                         min_samples_split = 2, min_samples_leaf = 1, min_weight_fraction_leaf = 0)\n",
    "# DeepSurv\n",
    "structure = [ {'activation': 'relu', 'num_units': 116}, \n",
    "             {'activation': 'tanh', 'num_units': 32}, ] \n",
    "DeepSurv = NonLinearCoxPHModel(structure=structure)\n",
    "DeepSurv.fit(\n",
    "    X_train, Y_train, E_train, \n",
    "    lr=1e-3, \n",
    "    init_method='xav_uniform', \n",
    "    num_epochs=200,\n",
    "    dropout=0.8,\n",
    "    batch_normalization=True,\n",
    "    bn_and_dropout=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
