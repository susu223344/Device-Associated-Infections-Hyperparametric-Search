{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device-associated infection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择共同因素\n",
    "X_trainf=X_train\n",
    "\n",
    "X_testf=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_trainf)\n",
    "y = np.array(y_train)\n",
    "\n",
    "X_testff = np.array(X_testf)\n",
    "y_testff = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import  RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from scipy import interp\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from scipy import stats  \n",
    "from scipy.stats import norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = RandomForestClassifier()\n",
    "param_lst = {\"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "             \"max_depth\": [1, 1.5, 2],\n",
    "             \"n_estimators\": [50, 70, 100],\n",
    "             \"min_samples_split\": [0.5, 0.6, 0.7]\n",
    "             }\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_params_ \n",
    "## {'criterion': 'gini', 'max_depth': 1, 'min_samples_split': 0.5, 'n_estimators': 50}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = LogisticRegression()\n",
    "param_lst = {\"penalty\": ['l1', 'l2', 'elasticnet'],\n",
    "              \"max_iter\": [20,50.100],\n",
    "              \"C\": [0.2,0.5,1] \n",
    "             }\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_params_ \n",
    "## {'C': 0.2, 'max_iter': 20, 'penalty': 'l2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = SVC(probability= True)\n",
    "param_lst = {\"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              \"max_iter\": [-1,3,10],\n",
    "              #\"degree\": [1,3,5],\n",
    "              \"gamma\": [1,5,9],\n",
    "              \"C\": [0.5,0.8,1] \n",
    "             }\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_params_ \n",
    "## {'C': 0.5, 'gamma': 1, 'kernel': 'linear', 'max_iter': -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "param_lst = {\"max_depth\": [0, 6, 8],\n",
    "              \"min_child_weight\" : [0.7, 1, 2],\n",
    "              \"n_estimators\": [100, 150, 200],\n",
    "              \"learning_rate\": [0.3, 0.8, 0.9],\n",
    "              \"gamma\": [1, 3, 10] \n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_params_ \n",
    "## {'gamma': 10, 'learning_rate': 0.3, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "param_lst = {'var_smoothing': [1e-6, 1e-7, 1e-8, 1e-9,1e-10, 1e-11, 1e-12]} \n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_params_ \n",
    "## {'var_smoothing': 1e-06}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "param_lst = {\"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "             \"splitter\": ['best', 'random'],\n",
    "             \"max_depth\": [3, 5, 7],\n",
    "             \"min_samples_leaf\": [0.3, 0.5, 1],\n",
    "             \"min_samples_split\": [1, 3, 6]\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_params_ \n",
    "## {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 6, 'splitter': 'random'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(units=50, optimizer='adam', dropout=0.2):\n",
    "    lstm = Sequential() \n",
    "    lstm.add(LSTM(units=32, return_sequences=True, dropout=dropout, input_shape=(X_trainf.shape[1], 1)))\n",
    "    lstm.add(LSTM(units=units)) \n",
    "    lstm.add(Dense(10, activation='relu'))\n",
    "    lstm.add(Dense(1)) \n",
    "    lstm.compile(loss='mean_squared_error',\n",
    "                 optimizer=optimizer, metrics='mse')\n",
    "    return lstm\n",
    "\n",
    "\n",
    "lstm = KerasClassifier(build_fn=create_model, epochs=50, batch_size=64, verbose=0)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"units\": [10, 50],\n",
    "    \"optimizer\": ['adam', 'sgd'],\n",
    "    \"dropout\": [0.1, 0.3],\n",
    "    \"batch_size\": [64, 128],\n",
    "    \"epochs\": [10, 50]   \n",
    "}\n",
    " \n",
    "\n",
    "grid = GridSearchCV(estimator=lstm, param_grid=param_grid, n_jobs=-1,cv=10)\n",
    "grid_result = grid.fit(X_trainf, y_train)\n",
    " \n",
    "\n",
    "print(f\"Best mse: {grid_result.best_score_:.2f} using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best mse: nan using {'batch_size': 64, 'dropout': 0.1, 'epochs': 10, 'optimizer': 'adam', 'units': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30-day survival model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.util import Surv\n",
    "\n",
    "y_train_ = Surv.from_dataframe(\n",
    "    event='status', \n",
    "    time='survival_time', \n",
    "    data=y_train)\n",
    "\n",
    "y_test_ = Surv.from_dataframe(\n",
    "    event='status', \n",
    "    time='survival_time', \n",
    "    data=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = CoxPHSurvivalAnalysis()\n",
    "\n",
    "param_lst = {\"alpha\": [0, 0.5, 1],\n",
    "             \"ties\": ['breslow', 'efron'],\n",
    "             \"n_iter\" : [10, 20, 50],\n",
    "             #\"tol \": [1e-09, 1e-08]\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train_)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_params_ \n",
    "## {'alpha': 0, 'n_iter': 10, 'ties': 'efron'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sksurv.ensemble import ExtraSurvivalTrees\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = ExtraSurvivalTrees()\n",
    "\n",
    "param_lst = {\"n_estimators\": [10, 20, 50],\n",
    "             \"max_depth\": [8, 10, 12],\n",
    "             \"min_samples_split\": [5,6,7],\n",
    "             \"min_samples_leaf\": [3,6,12]\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train_)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_params_ \n",
    "## {'max_depth': 12, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 50}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.tree import SurvivalTree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "model = SurvivalTree()\n",
    "\n",
    "\n",
    "param_lst = {\"min_samples_split\": [2, 4, 6],\n",
    "             \"min_samples_leaf\": [1, 3, 5],\n",
    "             \"max_depth\": [0, 2, 4],\n",
    "             \"splitter\": ['best', 'random']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train_)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_params_ \n",
    "## {'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 4, 'splitter': 'random'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GradientBoostingSurvivalAnalysis()\n",
    "\n",
    "param_lst = {\"n_estimators\": [10, 20, 50],\n",
    "             \"min_samples_leaf\": [1, 3, 5],\n",
    "             \"max_depth\": [0, 2, 4],\n",
    "             \"min_samples_split\": [2, 4, 6]\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train_)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_params_ \n",
    "## {'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 50}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from pysurvival.models.simulations import SimulationModel\n",
    "from pysurvival.models.semi_parametric import NonLinearCoxPHModel\n",
    "from pysurvival.utils.metrics import concordance_index\n",
    "from pysurvival.utils.display import integrated_brier_score\n",
    "\n",
    "# Creating the X, Y and E input\n",
    "Y_train = y_train['survival_time'].values\n",
    "Y_test = y_test['survival_time'].values\n",
    "E_train = (y_train['status'].values).astype(int)\n",
    "E_test = (y_test['status'].values).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the MLP structure\n",
    "structure = [ {'activation': 'relu', 'num_units': 20}, #116\n",
    "             {'activation': 'tanh', 'num_units': 20}, ] #32\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [1, 0.1, 0.01], \n",
    "    'dropout_rate': [0.5, 0.6, 0.7],  \n",
    "    'number_of_epochs': [10, 50, 90],  \n",
    "    'batch_normalization_enabled': [True, False],  \n",
    "    'neural_network_structure': [\n",
    "        [{'activation': 'relu', 'num_units': 20}, {'activation': 'tanh', 'num_units': 20}],\n",
    "        [{'activation': 'relu', 'num_units': 30}, {'activation': 'tanh', 'num_units': 30}],\n",
    "        [{'activation': 'relu', 'num_units': 50}, {'activation': 'tanh', 'num_units': 50}]\n",
    "    ]  # MLP\n",
    "}\n",
    "\n",
    "# Building the model\n",
    "deepsurv = NonLinearCoxPHModel(structure=structure)\n",
    "\n",
    "\n",
    "kfold_cross_validation = KFold(n_splits=10)\n",
    "\n",
    "best_concordance_index_score = -np.inf\n",
    "best_hyperparameters = None\n",
    "best_trained_model = None\n",
    "\n",
    "\n",
    "for learning_rate in param_grid['learning_rate']:\n",
    "    for dropout_rate in param_grid['dropout_rate']:\n",
    "        for number_of_epochs in param_grid['number_of_epochs']:\n",
    "            for batch_normalization_enabled in param_grid['batch_normalization_enabled']:\n",
    "                for neural_network_structure in param_grid['neural_network_structure']:\n",
    "                    fold_concordance_index_scores = []\n",
    "                    for train_index, validation_index in kfold_cross_validation.split(X_train):\n",
    "                        X_train_fold, X_validation_fold = X_train.iloc[train_index], X_train.iloc[validation_index]\n",
    "                        y_train, y_validation = Y_train[train_index], Y_train[\n",
    "                            validation_index]\n",
    "                        e_train, e_validation = E_train[train_index], E_train[\n",
    "                            validation_index]\n",
    "\n",
    "                        \n",
    "                        nonlinear_cox_model = NonLinearCoxPHModel(structure=neural_network_structure)\n",
    "                        \n",
    "                        nonlinear_cox_model.fit(\n",
    "                            X_train_fold, y_train, e_train,\n",
    "                            lr=learning_rate,\n",
    "                            num_epochs=number_of_epochs,\n",
    "                            dropout=dropout_rate,\n",
    "                            batch_normalization=batch_normalization_enabled,\n",
    "                            optimizer ='sgd',\n",
    "                            verbose = 0\n",
    "                        )\n",
    "\n",
    "                        \n",
    "                        concordance_index_score = concordance_index(nonlinear_cox_model, X_validation_fold, y_validation,\n",
    "                                                                    e_validation)\n",
    "                        fold_concordance_index_scores.append(concordance_index_score)\n",
    "                    mean_concordance_index_score = np.mean(fold_concordance_index_scores)\n",
    "                    print(f\"Parameters: learning_rate={learning_rate}, dropout_rate={dropout_rate}, number_of_epochs={number_of_epochs}, batch_normalization_enabled={batch_normalization_enabled}, neural_network_structure={neural_network_structure}\")\n",
    "                    print(f\"Mean Concordance Index: {mean_concordance_index_score:.4f}\")\n",
    "\n",
    "                    \n",
    "                    if mean_concordance_index_score > best_concordance_index_score:\n",
    "                        best_concordance_index_score = mean_concordance_index_score\n",
    "                        best_hyperparameters = {'learning_rate': learning_rate, 'dropout_rate': dropout_rate, 'number_of_epochs': number_of_epochs,\n",
    "                                                'batch_normalization_enabled': batch_normalization_enabled, 'neural_network_structure': neural_network_structure}\n",
    "                        best_trained_model = nonlinear_cox_model\n",
    "\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "print(f\"Best Concordance Index: {best_concordance_index_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best hyperparameters: {'learning_rate': 0.01, 'dropout_rate': 0.5, 'number_of_epochs': 50, 'batch_normalization_enabled': False, 'neural_network_structure': [{'activation': 'relu', 'num_units': 30}, {'activation': 'tanh', 'num_units': 30}]}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
