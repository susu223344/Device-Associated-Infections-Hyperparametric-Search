{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device-associated infection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择共同因素\n",
    "X_trainf=X_train\n",
    "\n",
    "X_testf=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_trainf)\n",
    "y = np.array(y_train)\n",
    "\n",
    "X_testff = np.array(X_testf)\n",
    "y_testff = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import  RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from scipy import interp\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from scipy import stats  \n",
    "from scipy.stats import norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "RandomForestClassifier(max_depth=1, min_samples_split=0.5, n_estimators=50)\n",
      "{'criterion': 'gini', 'max_depth': 1, 'min_samples_split': 0.5, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = RandomForestClassifier()\n",
    "param_lst = {\"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "             \"max_depth\": [1, 1.5, 2],\n",
    "             \"n_estimators\": [50, 70, 100],\n",
    "             \"min_samples_split\": [0.5, 0.6, 0.7]\n",
    "             }\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "LogisticRegression(C=0.2, max_iter=20)\n",
      "{'C': 0.2, 'max_iter': 20, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "120 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "One or more of the test scores are non-finite: [       nan 0.94051026        nan        nan 0.9401772         nan\n",
      "        nan 0.94001026        nan        nan 0.93967693        nan\n",
      "        nan 0.93901026        nan        nan 0.93984332        nan]\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = LogisticRegression()\n",
    "param_lst = {\"penalty\": ['l1', 'l2', 'elasticnet'],\n",
    "              \"max_iter\": [20,50.100],\n",
    "              \"C\": [0.2,0.5,1] \n",
    "             }\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=0.5, gamma=1, kernel='linear', probability=True)\n",
      "{'C': 0.5, 'gamma': 1, 'kernel': 'linear', 'max_iter': -1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = SVC(probability= True)\n",
    "param_lst = {\"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              \"max_iter\": [-1,3,10],\n",
    "              #\"degree\": [1,3,5],\n",
    "              \"gamma\": [1,5,9],\n",
    "              \"C\": [0.5,0.8,1] \n",
    "             }\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 243 candidates, totalling 2430 fits\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=10, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n",
      "{'gamma': 10, 'learning_rate': 0.3, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "param_lst = {\"max_depth\": [0, 6, 8],\n",
    "              \"min_child_weight\" : [0.7, 1, 2],\n",
    "              \"n_estimators\": [100, 150, 200],\n",
    "              \"learning_rate\": [0.3, 0.8, 0.9],\n",
    "              \"gamma\": [1, 3, 10] \n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n",
      "GaussianNB(var_smoothing=1e-06)\n",
      "{'var_smoothing': 1e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "param_lst = {'var_smoothing': [1e-6, 1e-7, 1e-8, 1e-9,1e-10, 1e-11, 1e-12]} \n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_split=6,\n",
      "                       splitter='random')\n",
      "{'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 6, 'splitter': 'random'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "900 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "540 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\tree\\_classes.py\", line 254, in fit\n",
      "    % self.min_samples_split\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "360 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\tree\\_classes.py\", line 942, in fit\n",
      "    X_idx_sorted=X_idx_sorted,\n",
      "  File \"C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "One or more of the test scores are non-finite: [       nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.93984387 0.93984359 0.9396772  0.93967665\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.93467831 0.93884387 0.93501165 0.93817693\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.92834637 0.93567804 0.92668081 0.93434415\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.93884581 0.9396761  0.93867915 0.9406761\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.93484748 0.93900998 0.93418081 0.93634443\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.94034332 0.94034332 0.94034332 0.94034332\n",
      "        nan        nan 0.93168192 0.93467748 0.9305147  0.93401165\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "param_lst = {\"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "             \"splitter\": ['best', 'random'],\n",
    "             \"max_depth\": [3, 5, 7],\n",
    "             \"min_samples_leaf\": [0.3, 0.5, 1],\n",
    "             \"min_samples_split\": [1, 3, 6]\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_lst, cv=10, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mse: nan using {'batch_size': 64, 'dropout': 0.1, 'epochs': 10, 'optimizer': 'adam', 'units': 10}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(units=50, optimizer='adam', dropout=0.2):\n",
    "    lstm = Sequential() \n",
    "    lstm.add(LSTM(units=32, return_sequences=True, dropout=dropout, input_shape=(X_trainf.shape[1], 1)))\n",
    "    lstm.add(LSTM(units=units)) \n",
    "    lstm.add(Dense(10, activation='relu'))\n",
    "    lstm.add(Dense(1)) \n",
    "    lstm.compile(loss='mean_squared_error',\n",
    "                 optimizer=optimizer, metrics='mse')\n",
    "    return lstm\n",
    "\n",
    "\n",
    "lstm = KerasClassifier(build_fn=create_model, epochs=50, batch_size=64, verbose=0)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"units\": [10, 50],\n",
    "    \"optimizer\": ['adam', 'sgd'],\n",
    "    \"dropout\": [0.1, 0.3],\n",
    "    \"batch_size\": [64, 128],\n",
    "    \"epochs\": [10, 50]   \n",
    "}\n",
    " \n",
    "\n",
    "grid = GridSearchCV(estimator=lstm, param_grid=param_grid, n_jobs=-1,cv=10)\n",
    "grid_result = grid.fit(X_trainf, y_train)\n",
    " \n",
    "\n",
    "print(f\"Best mse: {grid_result.best_score_:.2f} using {grid_result.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
